{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.functions as fn\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definicion de Variables\n",
    "config_file = \"config_encrypted.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_csv(df, file_path):\n",
    "    \"\"\"Saves the DataFrame to a CSV file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    df.to_csv(file_path, index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "    \n",
    "    if os.path.exists(file_path):  # Cambiar 'csv_path' a 'file_path'\n",
    "        print(f'    Archivo guardado exitosamente: {file_path}')\n",
    "    else:\n",
    "        print(f'    Error al guardar el archivo: {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_health(df, system, instance, config, csvPath):\n",
    "#     \"\"\"Process the Health DataFrame.\"\"\"\n",
    "    \n",
    "#     # Posibles categorías de salud\n",
    "#     possible_health_category = ['CONFIGURATION', 'DATA_PROTECTION', 'PERFORMANCE', 'COMPONENTS', 'CAPACITY']\n",
    "\n",
    "#     # Crear un DataFrame base con todas las categorías de salud y valores iniciales\n",
    "#     df_health_base = pd.DataFrame({\n",
    "#         'healthCategory': possible_health_category,\n",
    "#         'Score': 0,\n",
    "#         'Issues': 0\n",
    "#     })\n",
    "\n",
    "#     # Agrupar por healthCategory y sumar los scoreDeduction y contar los issues\n",
    "#     df_health_grouped = df.groupby('healthCategory').agg({\n",
    "#         'scoreDeduction': 'max',\n",
    "#         'healthCategory': 'count'\n",
    "#     }).rename(columns={'scoreDeduction': 'Score', 'healthCategory': 'Issues'})\n",
    "#     # Convertir los valores de Score a negativos\n",
    "#     df_health_grouped['Score'] = -df_health_grouped['Score']\n",
    "\n",
    "#     # Unir los datos para completar el DataFrame de salud\n",
    "#     df_health = pd.merge(df_health_base, df_health_grouped, on='healthCategory', how='outer', suffixes=('_base', '_grouped'))\n",
    "#     # Rellenar los valores NaN con los valores correspondientes en 'Score' y 'Issues'\n",
    "#     df_health['Score'] = df_health['Score_grouped'].fillna(df_health['Score_base']).astype(int)\n",
    "#     df_health['Issues'] = df_health['Issues_grouped'].fillna(df_health['Issues_base']).astype(int)\n",
    "#     df_health = df_health[['healthCategory', 'Issues', 'Score']]\n",
    "\n",
    "#     # Renombrar la columna 'healthCategory' a 'Health'\n",
    "#     df_health = df_health.rename(columns={'healthCategory': 'CATEGORY'})\n",
    "\n",
    "#     # Reemplazar los valores en la columna 'CATEGORY'\n",
    "#     category_mapping = {\n",
    "#         'CONFIGURATION': 'Configuration',\n",
    "#         'DATA_PROTECTION': 'Data Protection',\n",
    "#         'PERFORMANCE': 'Performance',\n",
    "#         'COMPONENTS': 'Components',\n",
    "#         'CAPACITY': 'Capacity'\n",
    "#     }\n",
    "#     df_health['CATEGORY'] = df_health['CATEGORY'].replace(category_mapping)\n",
    "\n",
    "#     ########## OBTENER TABLA df_health_system ###################\n",
    "#     # Calcular SystemScore: el menor valor en la columna 'Score'\n",
    "#     system_score = df_health['Score'].min()\n",
    "    \n",
    "#     # Convertir el menor valor negativo en la escala de 100 (100 + SystemScore)\n",
    "#     system_score_converted = 100 + system_score\n",
    "    \n",
    "#     # Calcular TotalNumIssues: suma total de los issues en la columna 'Issues'\n",
    "#     total_num_issues = df_health['Issues'].sum()\n",
    "    \n",
    "#     # Calcular STATUS basado en SystemScore convertido\n",
    "#     if system_score_converted > 95:\n",
    "#         status = \"GOOD\"\n",
    "#     elif 71 < system_score_converted <= 94:\n",
    "#         status = \"FAIR\"\n",
    "#     else:\n",
    "#         status = \"POOR\"\n",
    "\n",
    "#     # Crear un DataFrame con los resultados\n",
    "#     df_status = pd.DataFrame([{\n",
    "#         'TotalNumIssues': total_num_issues,\n",
    "#         'SystemScore': system_score_converted,\n",
    "#         'STATUS': status\n",
    "#     }])\n",
    "\n",
    "#     #############################################################\n",
    "\n",
    "\n",
    "#     # Reemplazar los valores en el DataFrame original de eventos de salud\n",
    "#     df_health_events = df.replace(r'\\n', '|||', regex=True)\n",
    "\n",
    "#     # Get file paths from config\n",
    "#     csv_files = config['systems'][system]['files']['csv']\n",
    "\n",
    "#     fn.save_dataframe_to_csv(df_health, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"dashboardHealh\"]}'))\n",
    "#     fn.save_dataframe_to_csv(df_health_events, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"healthEvents\"]}'))\n",
    "#     fn.save_dataframe_to_csv(df_status, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"healthSystem\"]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_health_summary(df):\n",
    "    \"\"\"Crear un resumen de métricas de salud.\"\"\"\n",
    "    health_categories = ['CONFIGURATION', 'DATA_PROTECTION', 'PERFORMANCE', 'COMPONENTS', 'CAPACITY']\n",
    "    \n",
    "    # Plantilla base con categorías de salud\n",
    "    df_health_template = pd.DataFrame({\n",
    "        'healthCategory': health_categories,\n",
    "        'Score': 0,\n",
    "        'Issues': 0\n",
    "    })\n",
    "\n",
    "    # Agrupar y calcular métricas\n",
    "    df_grouped_health_metrics = df.groupby('healthCategory').agg({\n",
    "        'scoreDeduction': 'max',\n",
    "        'healthCategory': 'count'\n",
    "    }).rename(columns={'scoreDeduction': 'Score', 'healthCategory': 'Issues'})\n",
    "    df_grouped_health_metrics['Score'] = -df_grouped_health_metrics['Score']\n",
    "\n",
    "    # Completar resumen de salud\n",
    "    df_health_summary = pd.merge(\n",
    "        df_health_template,\n",
    "        df_grouped_health_metrics,\n",
    "        on='healthCategory',\n",
    "        how='outer',\n",
    "        suffixes=('_template', '_grouped')\n",
    "    )\n",
    "    df_health_summary['Score'] = df_health_summary['Score_grouped'].fillna(df_health_summary['Score_template']).astype(int)\n",
    "    df_health_summary['Issues'] = df_health_summary['Issues_grouped'].fillna(df_health_summary['Issues_template']).astype(int)\n",
    "    df_health_summary = df_health_summary[['healthCategory', 'Issues', 'Score']]\n",
    "\n",
    "    # Renombrar y reemplazar valores\n",
    "    df_health_summary = df_health_summary.rename(columns={'healthCategory': 'CATEGORY'})\n",
    "    category_mapping = {\n",
    "        'CONFIGURATION': 'Configuration',\n",
    "        'DATA_PROTECTION': 'Data Protection',\n",
    "        'PERFORMANCE': 'Performance',\n",
    "        'COMPONENTS': 'Components',\n",
    "        'CAPACITY': 'Capacity'\n",
    "    }\n",
    "    df_health_summary['CATEGORY'] = df_health_summary['CATEGORY'].replace(category_mapping)\n",
    "    \n",
    "    return df_health_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_health_system_status(df_health_summary):\n",
    "    \"\"\"Crear el estado general del sistema.\"\"\"\n",
    "    lowest_health_score = df_health_summary['Score'].min()\n",
    "    normalized_system_score = 100 + lowest_health_score\n",
    "    total_issues_count = df_health_summary['Issues'].sum()\n",
    "\n",
    "    # Determinar estado basado en el puntaje\n",
    "    if normalized_system_score > 95:\n",
    "        system_status = \"GOOD\"\n",
    "    elif 71 < normalized_system_score <= 94:\n",
    "        system_status = \"FAIR\"\n",
    "    else:\n",
    "        system_status = \"POOR\"\n",
    "\n",
    "    # Crear DataFrame con el estado\n",
    "    df_system_status = pd.DataFrame([{\n",
    "        'TotalIssuesCount': total_issues_count,\n",
    "        'SystemScore': normalized_system_score,\n",
    "        'STATUS': system_status\n",
    "    }])\n",
    "\n",
    "    return df_system_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_health_events(df):\n",
    "    \"\"\"Limpiar y transformar registros de eventos de salud.\"\"\"\n",
    "    return df.replace(r'\\n', '|||', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_health(df, system, instance, config, csvPath):\n",
    "    \"\"\"Procesar datos de salud y guardar los resultados en CSV.\"\"\"\n",
    "    # Crear los DataFrames requeridos\n",
    "    df_health_summary = create_health_summary(df)\n",
    "    df_system_status = create_health_system_status(df_health_summary)\n",
    "    df_event_logs = create_health_events(df)\n",
    "\n",
    "    # Guardar los DataFrames en archivos CSV\n",
    "    csv_files = config['systems'][system]['files']['csv']\n",
    "    \n",
    "    fn.save_dataframe_to_csv(df_health_summary, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"healthSummary\"]}'))\n",
    "    fn.save_dataframe_to_csv(df_event_logs, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"healthEvents\"]}'))\n",
    "    fn.save_dataframe_to_csv(df_system_status, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"healthSystemStatus\"]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_job_group_activities(df, system, instance, config, csvPath):\n",
    "#     \"\"\"Process the Dashboard Job Group Activities DataFrame.\"\"\"\n",
    "\n",
    "\n",
    "#     # Filtrar el DataFrame por categorías específicas\n",
    "#     categories = ['CLOUD_TIER', 'INDEX', 'PROTECT', 'REPLICATE', 'RESTORE']\n",
    "#     df_filtered = df[df['category'].isin(categories)]\n",
    "\n",
    "#     # Create Job Groups DataFrame\n",
    "#     df_job_groups_summary = df_filtered['result.status'].value_counts().reset_index()\n",
    "#     df_job_groups_summary.columns = ['result.status', 'Num']\n",
    "\n",
    "#     # # Complete Job Groups DataFrame including all possible result.status\n",
    "#     possible_result_status = ['OK', 'FAILED', 'OK_WITH_ERRORS', 'CANCELED', 'SKIPPED', 'UNKNOWN']\n",
    "#     df_possible_result_status = pd.DataFrame({\n",
    "#         'result.status': possible_result_status,\n",
    "#         'Num': 0\n",
    "#     })\n",
    "\n",
    "#     # DataFrames, usando un merge outer para asegurarse de no perder datos\n",
    "#     df_job_groups_summary = pd.merge(df_possible_result_status, df_job_groups_summary, on='result.status', how='outer')\n",
    "#     # Llenar valores nulos en la columna Num\n",
    "#     df_job_groups_summary['Num'] = df_job_groups_summary['Num_y'].combine_first(df_job_groups_summary['Num_x']).astype(int)\n",
    "#     # Seleccionar solamente las columnas necesarias\n",
    "#     df_job_groups_summary = df_job_groups_summary[['result.status', 'Num']]\n",
    "\n",
    "#    # Reemplazar los valores de 'result.status' para adecuarlo al Dashboard de PPDM\n",
    "#     status_mapping = {\n",
    "#         'OK': 'Successful',\n",
    "#         'FAILED': 'Failed',\n",
    "#         'OK_WITH_ERRORS': 'Completed with Exceptions',\n",
    "#         'CANCELED': 'Canceled',\n",
    "#         'SKIPPED': 'Skipped',\n",
    "#         'UNKNOWN': 'Unknown'\n",
    "#     }\n",
    "\n",
    "#     df_job_groups_summary['result.status'] = df_job_groups_summary['result.status'].replace(status_mapping)\n",
    "\n",
    "#     # # Renombrar la columna 'result.status' a 'STATUS'\n",
    "#     df_job_groups_summary = df_job_groups_summary.rename(columns={'result.status': 'STATUS'})\n",
    "\n",
    "\n",
    "#     # Calcular TOTAL\n",
    "#     TOTAL = df_job_groups_summary['Num'].sum()\n",
    "    \n",
    "#     # Calcular RATE de 'Successful'\n",
    "#     successful_count = df_job_groups_summary.loc[df_job_groups_summary['STATUS'] == 'Successful', 'Num'].sum()\n",
    "#     RATE = round((successful_count / TOTAL) * 100, 2) if TOTAL > 0 else 0\n",
    "\n",
    "#     df_job_groups_rate = pd.DataFrame([[TOTAL, RATE]], columns=['Total Job Groups', 'Rate (%)'])\n",
    "\n",
    "\n",
    "#     # Get file paths from config\n",
    "#     csv_files = config['systems'][system]['files']['csv']\n",
    "    \n",
    "#     fn.save_dataframe_to_csv(df_job_groups_summary, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"dashboardjobgroupActivities\"]}'))\n",
    "#     fn.save_dataframe_to_csv(df_job_groups_rate, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"dashboardJobGroupRate\"]}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_job_group_status(df_filtered):\n",
    "    \"\"\"Crear un resumen de actividades por estado de resultado.\"\"\"\n",
    "    possible_statuses = ['OK', 'FAILED', 'OK_WITH_ERRORS', 'CANCELED', 'SKIPPED', 'UNKNOWN']\n",
    "    df_all_statuses = pd.DataFrame({'result_status': possible_statuses, 'Count': 0})\n",
    "\n",
    "    # Contar ocurrencias de cada estado de resultado\n",
    "    df_status_counts = df_filtered['result.status'].value_counts().reset_index()\n",
    "    df_status_counts.columns = ['result_status', 'Count']\n",
    "\n",
    "    # Completar con posibles estados faltantes\n",
    "    df_job_group_summary = pd.merge(\n",
    "        df_all_statuses,\n",
    "        df_status_counts,\n",
    "        on='result_status',\n",
    "        how='outer'\n",
    "    )\n",
    "    # Completar valores nulos con ceros\n",
    "    df_job_group_summary['Count'] = df_job_group_summary['Count_y'].combine_first(df_job_group_summary['Count_x']).astype(int)\n",
    "    df_job_group_summary = df_job_group_summary[['result_status', 'Count']]\n",
    "\n",
    "    # Mapear los estados de resultado a nombres amigables\n",
    "    status_mapping = {\n",
    "        'OK': 'Successful',\n",
    "        'FAILED': 'Failed',\n",
    "        'OK_WITH_ERRORS': 'Completed with Exceptions',\n",
    "        'CANCELED': 'Canceled',\n",
    "        'SKIPPED': 'Skipped',\n",
    "        'UNKNOWN': 'Unknown'\n",
    "    }\n",
    "    df_job_group_summary['result_status'] = df_job_group_summary['result_status'].replace(status_mapping)\n",
    "\n",
    "    # Renombrar columnas\n",
    "    df_job_group_summary = df_job_group_summary.rename(columns={'result_status': 'STATUS'})\n",
    "\n",
    "    return df_job_group_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_job_group_rate(df_job_group_summary):\n",
    "    \"\"\"Calcular el total y el porcentaje de éxito de los grupos de trabajos.\"\"\"\n",
    "    total_jobs = df_job_group_summary['Count'].sum()\n",
    "    successful_jobs = df_job_group_summary.loc[df_job_group_summary['STATUS'] == 'Successful', 'Count'].sum()\n",
    "    success_rate = round((successful_jobs / total_jobs) * 100, 2) if total_jobs > 0 else 0\n",
    "\n",
    "    df_job_group_rate = pd.DataFrame([[total_jobs, success_rate]], columns=['Total Job Groups', 'Rate (%)'])\n",
    "    return df_job_group_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job_group_activities(df, system, instance, config, csv_path):\n",
    "    \"\"\"Procesar actividades del grupo de trabajos y guardar resultados en CSV.\"\"\"\n",
    "    # Filtrar el DataFrame para categorías relevantes\n",
    "    relevant_categories = ['CLOUD_TIER', 'INDEX', 'PROTECT', 'REPLICATE', 'RESTORE']\n",
    "    df_filtered_jobs = df[df['category'].isin(relevant_categories)]\n",
    "\n",
    "    # Crear los DataFrames de resumen y tasa de éxito\n",
    "    df_job_group_summary = summarize_job_group_status(df_filtered_jobs)\n",
    "    df_job_group_rate = calculate_job_group_rate(df_job_group_summary)\n",
    "\n",
    "    # Obtener rutas de archivos desde la configuración\n",
    "    csv_files = config['systems'][system]['files']['csv']\n",
    "\n",
    "    # Guardar los DataFrames en archivos CSV\n",
    "    fn.save_dataframe_to_csv(df_job_group_summary, os.path.join(csv_path, f'{system}-{instance}-{csv_files[\"jobgroupSummary\"]}'))\n",
    "    fn.save_dataframe_to_csv(df_job_group_rate, os.path.join(csv_path, f'{system}-{instance}-{csv_files[\"jobgroupRate\"]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_activities_no_ok(df, system, instance, config, csvPath):\n",
    "\n",
    "#################TABLA RESUMEN JOBS SKIPPED  #########################\n",
    "    # Filtrar entradas con result.status igual a \"SKIPPED\"\n",
    "    skipped_df = df[df['result.status'] == 'SKIPPED'].copy()\n",
    "    \n",
    "    # Reemplazar valores NaN con un string vacío para evitar errores\n",
    "    skipped_df['host.name'] = skipped_df['host.name'].fillna('')\n",
    "    \n",
    "    # Realizar la agrupación en dos pasos para mayor claridad\n",
    "    # Primero, crear un DataFrame con los nombres de hosts concatenados\n",
    "    hosts_df = skipped_df.groupby([\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code'\n",
    "    ])['host.name'].agg(\n",
    "        host_names=lambda x: ' / '.join(sorted(set(x))) if any(x) else ''\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Luego, contar hosts y assets únicos\n",
    "    counts_df = skipped_df.groupby([\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code'\n",
    "    ]).agg({\n",
    "        'host.name': lambda x: len(set(x.dropna())),\n",
    "        'asset.name': lambda x: len(set(x.dropna()))\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Renombrar columnas de counts_df\n",
    "    counts_df.columns = [\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code', \n",
    "        'num.hosts', \n",
    "        'num.assets'\n",
    "    ]\n",
    "    \n",
    "    # Combinar los DataFrames\n",
    "    final_df = hosts_df.merge(counts_df, on=[\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code'\n",
    "    ])\n",
    "    \n",
    "    # Reordenar columnas para asegurar el orden correcto\n",
    "    final_columns = [\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code', \n",
    "        'host_names', \n",
    "        'num.hosts', \n",
    "        'num.assets'\n",
    "    ]\n",
    "    \n",
    "      \n",
    "    df_summary_jobs_skipped = final_df[final_columns]\n",
    "####################TABLA RESUMEN JOBS SKIPPED  ##################################\n",
    "\n",
    "\n",
    "#################TABLA CON ERRORES DE JOBS (NO SKIPPED)  #########################\n",
    "    # Filtrar entradas que no sean \"SKIPPED\"\n",
    "    filtered_df = df[df['result.status'] != 'SKIPPED']\n",
    "\n",
    "    filtered_df = filtered_df.fillna(\"(empty)\")\n",
    "\n",
    "    # Calcular las ocurrencias\n",
    "    occurrence_df = filtered_df.groupby([\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code', \n",
    "        'host.name', \n",
    "        'asset.name',\n",
    "        'result.error.reason'\n",
    "    ]).size().reset_index(name='occurrence')\n",
    "\n",
    "    # Crear un nuevo DataFrame con las columnas seleccionadas (excepto 'occurrence')\n",
    "    selected_columns = [\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code',\n",
    "        'activityInitiatedType', \n",
    "        'host.name', \n",
    "        'asset.name', \n",
    "        'result.error.reason', \n",
    "        'result.error.extendedReason', \n",
    "        'result.error.detailedDescription', \n",
    "        'result.error.remediation'\n",
    "    ]\n",
    "    processed_df = filtered_df[selected_columns]\n",
    "\n",
    "    # Unir el DataFrame procesado con las ocurrencias\n",
    "    final_df = processed_df.drop_duplicates(subset=[\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code', \n",
    "        'host.name', \n",
    "        'asset.name',\n",
    "        'result.error.reason'\n",
    "    ])\n",
    "\n",
    "    final_df = final_df.merge(occurrence_df, on=[\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code', \n",
    "        'host.name', \n",
    "        'asset.name',\n",
    "        'result.error.reason'\n",
    "    ])\n",
    "\n",
    "    # Reordenar las columnas para que 'occurrence' esté en la posición deseada\n",
    "    final_columns = [\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code',\n",
    "        'activityInitiatedType' ,\n",
    "        'occurrence',\n",
    "        'host.name', \n",
    "        'asset.name', \n",
    "        'result.error.reason', \n",
    "        'result.error.extendedReason', \n",
    "        'result.error.detailedDescription', \n",
    "        'result.error.remediation'\n",
    "    ]\n",
    "    final_df = final_df[final_columns]\n",
    "\n",
    "    # Ordenar el DataFrame final\n",
    "    final_df_sorted = final_df.sort_values([\n",
    "        'category', \n",
    "        'protectionPolicy.name', \n",
    "        'result.status', \n",
    "        'result.error.code', \n",
    "        'host.name', \n",
    "        'asset.name',\n",
    "        'result.error.reason'\n",
    "    ])\n",
    "\n",
    "    df_unique_errors_no_skipped = final_df_sorted\n",
    "\n",
    "####################################################################\n",
    "\n",
    "    # susstituir la cadena \"\\n\" por \"|||\" en todo el contenido del DataFrame\n",
    "    #df_unique_errors_sorted = df_unique_errors_sorted.replace(r'\\n', '  ..  ', regex=True)\n",
    "    df_unique_errors_no_skipped = df_unique_errors_no_skipped.replace(r'\\n', '  ..  ', regex=True)\n",
    "\n",
    "    # Get file paths from config\n",
    "    csv_files = config['systems'][system]['files']['csv']\n",
    "\n",
    "    # Save DataFrames as CSV\n",
    "    # save_dataframe_to_csv(df_assets_with_errors, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"assetErrors\"]}'))\n",
    "    # save_dataframe_to_csv(df_hosts_with_errors, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"hostErrors\"]}'))\n",
    "    # save_dataframe_to_csv(df_unique_errors_sorted, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"jobErrors\"]}'))\n",
    "    fn.save_dataframe_to_csv(df_unique_errors_no_skipped, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"uniqueErrorsNoSkipped\"]}'))\n",
    "    fn.save_dataframe_to_csv(df_summary_jobs_skipped, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"summaryJobsSkipped\"]}'))\n",
    "    #save_dataframe_to_csv(final_df, os.path.join(base_path, f'{system}-{instance}-{csv_files[\"summaryJobsSkipped\"]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_storage_systems(df, system, instance, config, csvPath):\n",
    "    \"\"\"Process storage systems information from JSON data.\"\"\"\n",
    "    \n",
    "    # Filter for DATA_DOMAIN_SYSTEM entries\n",
    "    df_storage_systems = df[df['type'] == 'DATA_DOMAIN_SYSTEM']\n",
    "    \n",
    "    # Prepare a list to store processed rows\n",
    "    processed_rows = []\n",
    "    \n",
    "    # Iterate through the filtered DataFrame\n",
    "    for _, row in df_storage_systems.iterrows():\n",
    "        # Extract base information\n",
    "        name = row.get('name', '')\n",
    "        readiness = row.get('readiness', '').lower()\n",
    "        \n",
    "        # Check if dataDomain and capacities exist\n",
    "        capacities = row.get('details', {}).get('dataDomain', {}).get('capacities', [])\n",
    "        \n",
    "        # Process each capacity block\n",
    "        for capacity in capacities:\n",
    "            processed_rows.append({\n",
    "                'NAME': name,\n",
    "                'READINESS': readiness,\n",
    "                'TIER': capacity.get('type', ''),\n",
    "                # 'SIZE': capacity.get('totalPhysicalSize', ''),\n",
    "                # 'USED': capacity.get('totalPhysicalUsed', ''),\n",
    "                'PERCENT USED': f\"{capacity.get('percentUsed', 0):.2f}\",  # Format to 2 decimal places\n",
    "                'STATUS': capacity.get('capacityStatus', '')\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from processed rows\n",
    "    df_storage_systems_output = pd.DataFrame(processed_rows)\n",
    "    \n",
    "    df_storage_systems_output = df_storage_systems_output.sort_values(by=['NAME', 'TIER'])\n",
    "\n",
    "\n",
    "    # Get file paths from config\n",
    "    csv_files = config['systems'][system]['files']['csv']\n",
    "    \n",
    "    # Save to CSV\n",
    "    fn.save_dataframe_to_csv(df_storage_systems_output, os.path.join(csvPath, f'{system}-{instance}-{csv_files[\"storageSystems\"]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_if_not_empty(file_path, process_function, system, instance, config, csvPath):\n",
    "    \"\"\"Checks if the JSON data is empty; if not, converts it to a DataFrame and processes it.\"\"\"\n",
    "    data = fn.load_json_file(file_path)\n",
    "    if not data:\n",
    "        print(f'El archivo \"{file_path}\" está vacío o no contiene datos válidos. Se omitirá.')\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    process_function(df, system, instance, config, csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function that coordinates all tasks.\"\"\"\n",
    "    # Load configuration\n",
    "    with open(\"config_encrypted.json\", \"r\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    # Obtener la ruta base y la ruta de los ficheros JSON\n",
    "    base_path = config[\"basePath\"]  # Obtener la ruta base desde el archivo de configuración\n",
    "    json_relative_path = config[\"jsonPath\"]\n",
    "    csv_relative_path = config[\"csvPath\"]\n",
    "    jsonPath = os.path.join(base_path, json_relative_path)\n",
    "    csvPath = os.path.join(base_path, csv_relative_path)\n",
    "    \n",
    "    for system, system_config in config[\"systems\"].items():\n",
    "        if system != \"PPDM\":\n",
    "            continue  # Saltar este sistema si no es \"PPDM\"\n",
    "\n",
    "       \n",
    "        json_files = system_config['files']['json']\n",
    "        \n",
    "        for instance_config in system_config['instances']:\n",
    "            hostname = instance_config[\"hostname\"]\n",
    "            print('------------------------')\n",
    "            print(f'PROCESANDO SISTEMAS \"{system}\"')\n",
    "            print('------------------------')\n",
    "\n",
    "            print('------------------------')\n",
    "            print(f'Procesando información de : \"{hostname}\"')\n",
    "            print('------------------------')\n",
    "\n",
    "            # Process Health Issues\n",
    "            health_files = glob.glob(os.path.join(jsonPath, f'{system}-{hostname}-{json_files[\"systemHealthIssues\"]}')) \n",
    "            if not health_files:\n",
    "                print(f'  No existe el fichero \"{system}-{hostname}-{json_files[\"systemHealthIssues\"]}\"')           \n",
    "            else:\n",
    "                print(f'  {hostname}: Procesando fichero: {health_files}')\n",
    "                for file_path in health_files:\n",
    "                    process_if_not_empty(file_path, process_health, system, hostname, config, csvPath)\n",
    "            \n",
    "            # Process Job Group Activities\n",
    "            job_files = glob.glob(os.path.join(jsonPath, f'{system}-{hostname}-{json_files[\"jobGroupActivitiesSummary\"]}'))\n",
    "            if not job_files:\n",
    "                print(f'  No existe el fichero \"{system}-{hostname}-{json_files[\"jobGroupActivitiesSummary\"]}\"')\n",
    "            else:\n",
    "                print(f'  {hostname}: Procesando fichero: {job_files}')\n",
    "                for file_path in job_files:\n",
    "                    process_if_not_empty(file_path, process_job_group_activities, system, hostname, config, csvPath)\n",
    "            \n",
    "            # Process Activities No OK\n",
    "            activities_files = glob.glob(os.path.join(jsonPath, f'{system}-{hostname}-{json_files[\"activitiesNotOK\"]}'))            \n",
    "            if not activities_files:\n",
    "                print(f'  No existe el fichero \"{system}-{hostname}-{json_files[\"activitiesNotOK\"]}\"')\n",
    "            else:\n",
    "                print(f'  {hostname}: Procesando fichero: {activities_files}')\n",
    "                for file_path in activities_files:\n",
    "                    process_if_not_empty(file_path, process_activities_no_ok, system, hostname, config, csvPath)\n",
    "\n",
    "            # Process Storage Systems\n",
    "            storage_files = glob.glob(os.path.join(jsonPath, f'{system}-{hostname}-{json_files[\"storageSystems\"]}'))\n",
    "            if not storage_files:\n",
    "                print(f'  No existe el fichero \"{system}-{hostname}-{json_files[\"storageSystems\"]}\"')\n",
    "            else:\n",
    "                print(f'  {hostname}: Procesando fichero: {storage_files}')\n",
    "                for file_path in storage_files:\n",
    "                    process_if_not_empty(file_path, process_storage_systems, system, hostname, config, csvPath)\n",
    "            \n",
    "            print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "PROCESANDO SISTEMAS \"PPDM\"\n",
      "------------------------\n",
      "------------------------\n",
      "Procesando información de : \"PPDM-01.demo.local\"\n",
      "------------------------\n",
      "  PPDM-01.demo.local: Procesando fichero: ['E:\\\\DC\\\\dailycheck-PPDM\\\\jsonFiles\\\\PPDM-PPDM-01.demo.local-system_health_issues.json']\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-PPDM-01.demo.local-health_summary.csv\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-PPDM-01.demo.local-health_events.csv\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-PPDM-01.demo.local-health_system_status.csv\n",
      "  PPDM-01.demo.local: Procesando fichero: ['E:\\\\DC\\\\dailycheck-PPDM\\\\jsonFiles\\\\PPDM-PPDM-01.demo.local-JobGroup_activities_summary.json']\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-PPDM-01.demo.local-jobgroup_activities_summary.csv\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-PPDM-01.demo.local-jobgroup_activities_rate.csv\n",
      "  PPDM-01.demo.local: Procesando fichero: ['E:\\\\DC\\\\dailycheck-PPDM\\\\jsonFiles\\\\PPDM-PPDM-01.demo.local-activitiesNotOK.json']\n",
      "El archivo \"E:\\DC\\dailycheck-PPDM\\jsonFiles\\PPDM-PPDM-01.demo.local-activitiesNotOK.json\" está vacío o no contiene datos válidos. Se omitirá.\n",
      "  PPDM-01.demo.local: Procesando fichero: ['E:\\\\DC\\\\dailycheck-PPDM\\\\jsonFiles\\\\PPDM-PPDM-01.demo.local-storage_systems.json']\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-PPDM-01.demo.local-storage_systems.csv\n",
      "------------------------\n",
      "------------------------\n",
      "PROCESANDO SISTEMAS \"PPDM\"\n",
      "------------------------\n",
      "------------------------\n",
      "Procesando información de : \"192.168.1.15\"\n",
      "------------------------\n",
      "  192.168.1.15: Procesando fichero: ['E:\\\\DC\\\\dailycheck-PPDM\\\\jsonFiles\\\\PPDM-192.168.1.15-system_health_issues.json']\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-192.168.1.15-health_summary.csv\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-192.168.1.15-health_events.csv\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-192.168.1.15-health_system_status.csv\n",
      "  192.168.1.15: Procesando fichero: ['E:\\\\DC\\\\dailycheck-PPDM\\\\jsonFiles\\\\PPDM-192.168.1.15-JobGroup_activities_summary.json']\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-192.168.1.15-jobgroup_activities_summary.csv\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-192.168.1.15-jobgroup_activities_rate.csv\n",
      "  192.168.1.15: Procesando fichero: ['E:\\\\DC\\\\dailycheck-PPDM\\\\jsonFiles\\\\PPDM-192.168.1.15-activitiesNotOK.json']\n",
      "El archivo \"E:\\DC\\dailycheck-PPDM\\jsonFiles\\PPDM-192.168.1.15-activitiesNotOK.json\" está vacío o no contiene datos válidos. Se omitirá.\n",
      "  192.168.1.15: Procesando fichero: ['E:\\\\DC\\\\dailycheck-PPDM\\\\jsonFiles\\\\PPDM-192.168.1.15-storage_systems.json']\n",
      "    File saved succesfully: E:\\DC\\dailycheck-PPDM\\csvFiles\\PPDM-192.168.1.15-storage_systems.csv\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
