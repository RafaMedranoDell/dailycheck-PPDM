{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json_file(file_path):\n",
    "    \"\"\"Opens a JSON file and loads it into a dictionary.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(data):\n",
    "    \"\"\"Converts a dictionary to a pandas DataFrame.\"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_csv(df, file_name):\n",
    "    \"\"\"Saves the DataFrame to a CSV file.\"\"\"\n",
    "    csv_path = file_name\n",
    "    df.to_csv(csv_path, index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_health(df, system, instance):\n",
    "    \"\"\"Process the Health DataFrame.\"\"\"\n",
    "    \n",
    "    # Possible health categories\n",
    "    possible_health_category = ['CAPACITY', 'PERFORMANCE', 'CONFIGURATION', 'COMPONENTS', 'DATA_PROTECTION']\n",
    "\n",
    "    # Create a DataFrame with all possible 'healthCategory' set to '0'\n",
    "    df_health_base = pd.DataFrame({\n",
    "        'healthCategory': possible_health_category,\n",
    "        'Score': 0,\n",
    "        'Issues': 0\n",
    "    })\n",
    "\n",
    "    # Group by healthCategory and sum scoreDeduction and count the issues\n",
    "    df_health_grouped = df.groupby('healthCategory').agg({\n",
    "        'scoreDeduction': 'sum',\n",
    "        'healthCategory': 'count'\n",
    "    }).rename(columns={'scoreDeduction': 'Score', 'healthCategory': 'Issues'})\n",
    "    # Convierte los valores de Score a negativos\n",
    "    df_health_grouped['Score'] = -df_health_grouped['Score']\n",
    "\n",
    "    # Merge the data for a complete health DataFrame using 'health.category' ase key\n",
    "    df_health = pd.merge(df_health_base, df_health_grouped, on='healthCategory', how='outer', suffixes=('_base', '_grouped'))\n",
    "    # Rellena los valores NaN con los valores correspondientes en 'Score' y 'Issues'\n",
    "    df_health['Score'] = df_health['Score_grouped'].fillna(df_health['Score_base']).astype(int)\n",
    "    df_health['Issues'] = df_health['Issues_grouped'].fillna(df_health['Issues_base']).astype(int)\n",
    "    df_health = df_health[['healthCategory', 'Score', 'Issues']]\n",
    "\n",
    "    # Selecciona solamente las columnas necesarias y ajusta los nombres\n",
    "    df_health_events = df.replace(r'\\n', '|||', regex=True)\n",
    "\n",
    "    save_dataframe_to_csv(df_health, f'{system}-{instance}-Dashboard-Health.csv')\n",
    "    save_dataframe_to_csv(df_health_events, f'{system}-{instance}-Health_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job_group_activities(df, system, instance):\n",
    "    \"\"\"Process the Dashboard Job Group Activities DataFrame.\"\"\"\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    categories = ['CLOUD_TIER', 'INDEX', 'PROTECT', 'REPLICATE', 'RESTORE']\n",
    "    filtered_df_job_groups = df.loc[df['category'].isin(categories)]\n",
    "    \n",
    "    # Create Job Groups DataFrame\n",
    "    df_job_groups = filtered_df_job_groups['result.status'].value_counts().reset_index()\n",
    "    df_job_groups.columns = ['result.status', 'Num']\n",
    "\n",
    "    # Complete Job Groups DataFrame including all possible result.status\n",
    "    possible_result_status = ['OK', 'FAILED', 'OK_WITH_ERRORS', 'CANCELED', 'SKIPPED', 'UNKNOWN']\n",
    "    df_base = pd.DataFrame({\n",
    "        'result.status': possible_result_status,\n",
    "        'Num': 0\n",
    "    })\n",
    "\n",
    "    # DataFrames, usando un merge outer para asegurarse de no perder datos\n",
    "    df_job_groups_complete = pd.merge(df_base, df_job_groups, on='result.status', how='outer')\n",
    "    # Llenar valores nulos en la columna Num\n",
    "    df_job_groups_complete['Num'] = df_job_groups_complete['Num_y'].combine_first(df_job_groups_complete['Num_x']).astype(int)\n",
    "    # Seleccionar solamente las columnas necesarias\n",
    "    df_job_groups_complete = df_job_groups_complete[['result.status', 'Num']]\n",
    "\n",
    "    save_dataframe_to_csv(df_job_groups_complete, f'{system}-{instance}-Dashboard-JobGroupActivities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_activities_no_ok(df, system, instance):\n",
    "    \"\"\"Process the Activities No OK DataFrame.\"\"\"\n",
    "    \n",
    "    # Create DataFrame with number of ASSETS with ERRORS\n",
    "    df_assets_with_errors = df.groupby(\n",
    "        [\"category\", \"result.status\", \"result.error.code\",\"protectionPolicy.name\", \"asset.type\"]\n",
    "    ).agg(Count=(\"asset.name\", \"nunique\")).reset_index()\n",
    "\n",
    "    # Create DataFrame with number of HOSTS with ERRORS\n",
    "    df_hosts_with_errors = df.groupby(\n",
    "        [\"category\", \"result.status\", \"result.error.code\",\"protectionPolicy.name\"]\n",
    "    ).agg(Count=(\"host.name\", \"nunique\")).reset_index()\n",
    "\n",
    "    # Create DataFrame with all unique errors\n",
    "    columns_errors = ['category', 'result.status', 'protectionPolicy.name', 'result.error.code', 'host.name', 'asset.name', 'inventorySource.type', 'result.error.detailedDescription', 'result.error.reason','result.error.extendedReason', 'result.error.remediation']\n",
    "    df_errors = df[columns_errors]\n",
    "    df_unique_errors = df_errors.drop_duplicates(subset=['category', 'result.status', 'protectionPolicy.name', 'result.error.code', 'host.name', 'asset.name', 'inventorySource.type'])\n",
    "    \n",
    "    # susstituir la cadena \"\\n\" por \"|||\" en todo el contenido del DataFrame\n",
    "    df_unique_errors = df_unique_errors.replace(r'\\n', '|||', regex=True)\n",
    "\n",
    "    # Save DataFrames as CSV\n",
    "    save_dataframe_to_csv(df_assets_with_errors, f'{system}-{instance}-errorAssets.csv')\n",
    "    save_dataframe_to_csv(df_hosts_with_errors, f'{system}-{instance}-errorHosts.csv')\n",
    "    save_dataframe_to_csv(df_unique_errors, f'{system}-{instance}-jobErrors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function that coordinates all tasks.\"\"\"\n",
    "    \n",
    "    with open(\"config.json\", \"r\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "        print(config)\n",
    "\n",
    "    for system, instances in config[\"systems\"].items():\n",
    "        for instance in instances:\n",
    "            # Process Health Issues\n",
    "            print(instance)\n",
    "            health_files = glob.glob(f'{system}-{instance}-system_health_issues.json')\n",
    "            for file_path in health_files:\n",
    "                print(file_path)\n",
    "                data = open_json_file(file_path)\n",
    "                df = convert_to_dataframe(data)\n",
    "                process_health(df, system, instance)\n",
    "            \n",
    "            # Process Job Group Activities\n",
    "            job_files = glob.glob(f'{system}-{instance}-JobGroup_activities_summary.json')\n",
    "            for file_path in job_files:\n",
    "                data = open_json_file(file_path)\n",
    "                df = convert_to_dataframe(data)\n",
    "                process_job_group_activities(df, system, instance)\n",
    "            \n",
    "            # Process Activities No OK\n",
    "            activities_files = glob.glob(f'{system}-{instance}-activitiesNotOK.json')\n",
    "            for file_path in activities_files:\n",
    "                data = open_json_file(file_path)\n",
    "                df = convert_to_dataframe(data)\n",
    "                process_activities_no_ok(df, system, instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'systems': {'PPDM': ['PPDM-01', 'PPDM-01.demo.local']}}\n",
      "PPDM-01\n",
      "PPDM-PPDM-01-system_health_issues.json\n",
      "PPDM-01.demo.local\n",
      "PPDM-PPDM-01.demo.local-system_health_issues.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
